{
    "nbformat_minor": 1, 
    "cells": [
        {
            "source": "<table style=\"border: none\" align=\"center\">\n   <tr style=\"border: none\">\n      <th style=\"border: none\"><font face=\"verdana\" size=\"4\" color=\"black\"><b>Network Intrusion Detection</b></font></th>\n      <th style=\"border: none\"><img src=\"https://github.com/pmservice/customer-satisfaction-prediction/blob/master/app/static/images/ml_icon_gray.png?raw=true\" alt=\"Watson Machine Learning icon\" height=\"40\" width=\"40\"></th>\n   </tr> \n   <tr style=\"border: none\">\n       <td style=\"border: none\"><img src=\"https://github.com/pmservice/wml-sample-models/raw/master/tensorflow/hand-written-digit-recognition/images/experiment_banner.png\" width=\"600\" height = \"200\" alt=\"Icon\"></td>\n   </tr>\n</table>", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "This notebook contains steps and code to use Spark ML library to build classification models using kddcup data.\n\n**Notebook environment:** Scala 2.11 + Spark 2.2\n\n**Platform:** IBM Watson Studio\n\n**Dataset:**\n[UCI kddcup data](http://kdd.ics.uci.edu/databases/kddcup99/kddcup99.html) (743MB)\n\n**Purpose:**\nBuild algorithms that can detect the network intrusions.\n\n**Classification algorithms:**\n- Random Forest Classifier\n- Multilayer Perceptron Classifier\n\n**Contents: **\n\nThis notebook contains the following parts:\n\n1.\t[Download data](#download)\n2.\t[Load and prepare data](#load)\n3.\t[Building models](#model)\n  * [Random Forest](#rf)\n  * [MLP](#mlp)\n\n\n<a id=\"download\"></a>\n## Download data\nWe firstly download the zipped dataset(18M) to Watson's shared directory */opt/ibm/user-libs/shared-data*. If the \"shared-data\" folder doesn't exist, try to execute the commented code to create the folder.", 
            "cell_type": "markdown", 
            "attachments": {}, 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "import sys.process._\nimport java.net.URL\nimport java.io.File\n\n// \"mkdir /opt/ibm/user-libs/shared-data\".!\nval url = \"http://kdd.ics.uci.edu/databases/kddcup99/kddcup.data.gz\"\nval filename = \"/opt/ibm/user-libs/shared-data/kddcup.data.gz\"\nnew URL(url) #> new File(filename) !!"
        }, 
        {
            "source": "**Once we have the zipped dataset, use *gunzip* to unzip the file to the save directory. Use *ls* to see if the file is there.**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "\"gunzip /opt/ibm/user-libs/shared-data/kddcup.data.gz -d /opt/ibm/user-libs/shared-data/kddcup.data\".!\n\"ls /opt/ibm/user-libs/shared-data/\".!"
        }, 
        {
            "source": "<a id=\"load\"></a>\n## Load and prepare data\nThe data is comma splited, so we can directly use SparkSession to read the data as dataframe", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 1, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+---+---+----+---+---+-----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+\n|_c0|_c1| _c2|_c3|_c4|  _c5|_c6|_c7|_c8|_c9|_c10|_c11|_c12|_c13|_c14|_c15|_c16|_c17|_c18|_c19|_c20|_c21|_c22|_c23|_c24|_c25|_c26|_c27|_c28|_c29|_c30|_c31|_c32|_c33|_c34|_c35|_c36|_c37|_c38|_c39|_c40|   _c41|\n+---+---+----+---+---+-----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+\n|  0|tcp|http| SF|215|45076|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   1| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   0|   0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n|  0|tcp|http| SF|162| 4528|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   2|   2| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   1|   1| 1.0| 0.0| 1.0| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n|  0|tcp|http| SF|236| 1228|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   1|   1| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   2|   2| 1.0| 0.0| 0.5| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n|  0|tcp|http| SF|233| 2032|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   2|   2| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   3|   3| 1.0| 0.0|0.33| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n|  0|tcp|http| SF|239|  486|  0|  0|  0|  0|   0|   1|   0|   0|   0|   0|   0|   0|   0|   0|   0|   0|   3|   3| 0.0| 0.0| 0.0| 0.0| 1.0| 0.0| 0.0|   4|   4| 1.0| 0.0|0.25| 0.0| 0.0| 0.0| 0.0| 0.0|normal.|\n+---+---+----+---+---+-----+---+---+---+---+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+----+-------+\nonly showing top 5 rows\n\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "spark = org.apache.spark.sql.SparkSession@250ac6eb\ndf = [_c0: int, _c1: string ... 40 more fields]\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 1, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[_c0: int, _c1: string ... 40 more fields]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "import org.apache.spark.sql.SparkSession\n\nval spark = SparkSession.\n    builder().\n    getOrCreate()\nval df = spark.\n    read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").\n    option(\"inferSchema\", \"true\").\n    load(\"/opt/ibm/user-libs/shared-data/kddcup.data\")\ndf.show(5)\n"
        }, 
        {
            "source": "**Let's take a look at the schema and labels(the last column\"_c41\").**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 2, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+----------------+-------+\n|            _c41|  count|\n+----------------+-------+\n|    warezmaster.|     20|\n|          smurf.|2807886|\n|            pod.|    264|\n|           imap.|     12|\n|           nmap.|   2316|\n|   guess_passwd.|     53|\n|        ipsweep.|  12481|\n|      portsweep.|  10413|\n|          satan.|  15892|\n|           land.|     21|\n|     loadmodule.|      9|\n|      ftp_write.|      8|\n|buffer_overflow.|     30|\n|        rootkit.|     10|\n|    warezclient.|   1020|\n|       teardrop.|    979|\n|           perl.|      3|\n|            phf.|      4|\n|       multihop.|      7|\n|        neptune.|1072017|\n+----------------+-------+\nonly showing top 20 rows\n\n"
                }
            ], 
            "source": "df.printSchema"
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": "df.select(\"_c41\").groupBy(\"_c41\").count().show()"
        }, 
        {
            "source": "**According to the [description](http://kdd.ics.uci.edu/databases/kddcup99/training_attack_types), we should recode the labels into five categories. We can use a SQL query to do this. Name the new column name as \"label_s\" stands for *label in string*.**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 3, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+-------+-------+\n|label_s|  count|\n+-------+-------+\n|    u2r|     52|\n| normal| 972781|\n|    r2l|   1126|\n|  probe|  41102|\n|    dos|3883370|\n+-------+-------+\n\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "query = \n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "        WHEN 'warez...\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 3, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "SELECT *, \n    CASE _c41 \n        WHEN 'back.' THEN 'dos'\n        WHEN 'buffer_overflow.' THEN 'u2r'\n        WHEN 'ftp_write.' THEN 'r2l'\n        WHEN 'guess_passwd.' THEN 'r2l'\n        WHEN 'imap.' THEN 'r2l'\n        WHEN 'ipsweep.' THEN 'probe'\n        WHEN 'land.' THEN 'dos'\n        WHEN 'loadmodule.' THEN 'u2r'\n        WHEN 'multihop.' THEN 'r2l'\n        WHEN 'neptune.' THEN 'dos'\n        WHEN 'nmap.' THEN 'probe'\n        WHEN 'perl.' THEN 'u2r'\n        WHEN 'phf.' THEN 'r2l'\n        WHEN 'pod.' THEN 'dos'\n        WHEN 'portsweep.' THEN 'probe'\n        WHEN 'rootkit.' THEN 'u2r'\n        WHEN 'satan.' THEN 'probe'\n        WHEN 'smurf.' THEN 'dos'\n        WHEN 'spy.' THEN 'r2l'\n        WHEN 'teardrop.' THEN 'dos'\n        WHEN 'warezclient.' THEN 'r2l'\n        WHEN 'warezmaster.' THEN 'r2l'\n        ELSE 'normal'\nEND AS label_s \nFROM attack"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "df.createOrReplaceTempView(\"attack\")\n\nval query = \"\"\"SELECT *, \n    CASE _c41 \n        WHEN 'back.' THEN 'dos'\n        WHEN 'buffer_overflow.' THEN 'u2r'\n        WHEN 'ftp_write.' THEN 'r2l'\n        WHEN 'guess_passwd.' THEN 'r2l'\n        WHEN 'imap.' THEN 'r2l'\n        WHEN 'ipsweep.' THEN 'probe'\n        WHEN 'land.' THEN 'dos'\n        WHEN 'loadmodule.' THEN 'u2r'\n        WHEN 'multihop.' THEN 'r2l'\n        WHEN 'neptune.' THEN 'dos'\n        WHEN 'nmap.' THEN 'probe'\n        WHEN 'perl.' THEN 'u2r'\n        WHEN 'phf.' THEN 'r2l'\n        WHEN 'pod.' THEN 'dos'\n        WHEN 'portsweep.' THEN 'probe'\n        WHEN 'rootkit.' THEN 'u2r'\n        WHEN 'satan.' THEN 'probe'\n        WHEN 'smurf.' THEN 'dos'\n        WHEN 'spy.' THEN 'r2l'\n        WHEN 'teardrop.' THEN 'dos'\n        WHEN 'warezclient.' THEN 'r2l'\n        WHEN 'warezmaster.' THEN 'r2l'\n        ELSE 'normal'\nEND AS label_s \nFROM attack\"\"\"\nval labeled = spark.sql(query)\nlabeled.select(\"label_s\").groupBy(\"label_s\").count().show()"
        }, 
        {
            "source": "**Intuitively, we should split the data into *training* and *testing* sets before building ML pipeline. However, the number of categories for those categorical variables may be different between two sets. It will cause errors in building algorithms.**\n\nTherefore, we build a pipeline to prepare the data before splitting it to avoid errors.\n\n**The pipeline:**\n* StringIndexers: the c1, c2, c3 are categorical strings, we need to firstly index them\n* OneHotEncoders: after the categorical strings are indexed, we can now perform one-hot encoding to the indexed columns\n* VectorAssembler: Include the wanted columns and assemble them as a feature vector\n* labelIndexer: another StringIndexer to index the label_s column and output as \"label\" column", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 4, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "indexer1 = strIdx_2a69cf734c93\nindexer2 = strIdx_a24dff38fb3d\nindexer3 = strIdx_38a1e08311dd\nencoder1 = oneHot_8d9af096694e\nencoder2 = oneHot_800fa6c0dbb4\nencoder3 = oneHot_2aff1afb3a8f\nfeaturenames = Array(_c0, v_c1, v_c2, v_c3, _c4, _c5, _c6, _c7, _c8, _c9, _c10, _c11, _c12, _c13, _c14, _c15, _c16, _c17, _c18, _c19, _c22, _c23, _c24, _c25, _c26, _c27, _c28, _c29, _c30, _c31, _c32, _c33...\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 4, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[_c0, v_c1, v_c2, v_c3, _c4, _c5, _c6, _c7, _c8, _c9, _c10, _c11, _c12, _c13, _c14, _c15, _c16, _c17, _c18, _c19, _c22, _c23, _c24, _c25, _c26, _c27, _c28, _c29, _c30, _c31, _c32, _c33, _c34, _c35, _c36, _c37, _c38, _c39, _c40]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "import org.apache.spark.ml.feature.{IndexToString, StringIndexer, VectorAssembler, OneHotEncoder}\nimport org.apache.spark.ml.Pipeline\n\nval indexer1 = new StringIndexer()\n  .setInputCol(\"_c1\")\n  .setOutputCol(\"i_c1\")\n//   .setHandleInvalid(\"skip\")\n//   .fit(labeled)\nval indexer2 = new StringIndexer()\n  .setInputCol(\"_c2\")\n  .setOutputCol(\"i_c2\")\n//   .setHandleInvalid(\"skip\")\n//   .fit(labeled)\nval indexer3 = new StringIndexer()\n  .setInputCol(\"_c3\")\n  .setOutputCol(\"i_c3\")\n//   .setHandleInvalid(\"skip\")\n//   .fit(labeled)\n\nval encoder1 = new OneHotEncoder()\n  .setInputCol(\"i_c1\")\n  .setOutputCol(\"v_c1\")\n\nval encoder2 = new OneHotEncoder()\n  .setInputCol(\"i_c2\")\n  .setOutputCol(\"v_c2\")\n\nval encoder3 = new OneHotEncoder()\n  .setInputCol(\"i_c3\")\n  .setOutputCol(\"v_c3\")\n\nval featurenames = Array(\"_c0\", \"v_c1\", \"v_c2\", \"v_c3\", \"_c4\", \"_c5\", \"_c6\", \n                         \"_c7\", \"_c8\", \"_c9\", \"_c10\", \"_c11\", \"_c12\", \"_c13\", \n                         \"_c14\", \"_c15\", \"_c16\", \"_c17\", \"_c18\", \"_c19\",\n                         \"_c22\", \"_c23\", \"_c24\", \"_c25\", \"_c26\", \"_c27\", \n                         \"_c28\", \"_c29\", \"_c30\", \"_c31\", \"_c32\", \"_c33\", \"_c34\", \n                         \"_c35\", \"_c36\", \"_c37\", \"_c38\", \"_c39\", \"_c40\")\nval assembler = new VectorAssembler()\n  .setInputCols(featurenames)\n  .setOutputCol(\"features\")\n\nval labelIndexer = new StringIndexer()\n  .setInputCol(\"label_s\")\n  .setOutputCol(\"label\")\n//   .fit(labeled)\n\nval pipeline_prepare = new Pipeline()\n  .setStages(Array(indexer1,indexer2,indexer3,encoder1,encoder2,encoder3,assembler,labelIndexer))\n"
        }, 
        {
            "source": "**Fit and transform the data, select only \"label\" and \"features\" for training**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 5, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "prepare = pipeline_c4326f9e05c2\ndata = [label: double, features: vector]\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 5, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[label: double, features: vector]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "val prepare = pipeline_prepare.fit(labeled)\nval data = prepare.transform(labeled).select(\"label\",\"features\")"
        }, 
        {
            "source": "<a id=\"build\"></a>\n## Building models\n**Firstly we split the data, because the data is rather big, I choose 60% of the data to be training set and the rest goes to testing set**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 6, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "train = [label: double, features: vector]\ntest = [label: double, features: vector]\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 6, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[label: double, features: vector]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "val Array(train, test) = data.randomSplit(Array(0.6, 0.4))"
        }, 
        {
            "source": "<a id=\"rf\"></a>\n### Random Forest\nLet's start with random forest algorithm. Spark ML provides this algorithm and the only thing we need to do it set it up. One thing we need to notice is that there are 70 categories in c2 column, so the default _MaxBins_ is not enough (it has to be larger than the biggest number of categories of all categorical variables). We set the _MaxBins_ to be 72 to avoid errors. ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 7, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Training process takes 243.952950419 secs\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "t = 1487575431530246\nrf = rfc_50de58a56dcf\nrf_model = RandomForestClassificationModel (uid=rfc_50de58a56dcf) with 5 trees\nduration = 243.952950419\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 7, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "243.952950419"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "import org.apache.spark.ml.classification.{RandomForestClassificationModel, RandomForestClassifier}\nval t = System.nanoTime\nval rf = new RandomForestClassifier()\n  .setLabelCol(\"label\")\n  .setFeaturesCol(\"features\")\n  .setNumTrees(5)\n  .setMaxBins(72)\n\nval rf_model = rf.fit(train)\nval duration = (System.nanoTime - t) / 1e9d\nprintln(s\"Training process takes $duration secs\")"
        }, 
        {
            "source": "**It takes about 6 mins to train this model. Now we can check the model performance.**\n\n**Fit the model to test and get predictions**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 8, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "rf_predictions = [label: double, features: vector ... 3 more fields]\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 8, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "[label: double, features: vector ... 3 more fields]"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "val rf_predictions = rf_model.transform(test)"
        }, 
        {
            "source": "**Check the error and accuracy (The model is actually pretty good)**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 9, 
            "cell_type": "code", 
            "metadata": {
                "scrolled": true
            }, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Test Error of RF = 0.007372298792682086\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "evaluator = mcEval_d9c9f4bc9fe9\naccuracy = 0.9926277012073179\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 9, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.9926277012073179"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nval evaluator = new MulticlassClassificationEvaluator()\n  .setLabelCol(\"label\")\n  .setPredictionCol(\"prediction\")\n  .setMetricName(\"accuracy\")\nval accuracy = evaluator.evaluate(rf_predictions)\nprintln(\"Test Error of RF = \" + (1.0 - accuracy))"
        }, 
        {
            "source": "<a id=\"mlp\"></a>\n### Multilayer Perceptron Classifier\nBefore building the MLP model, we need to know how many nodes are needed for the input layer. Check the feature vector:", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 10, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "+--------------------+\n|            features|\n+--------------------+\n|(117,[0,2,5,72,82...|\n|(117,[0,2,5,72,82...|\n|(117,[0,2,5,72,82...|\n|(117,[0,2,5,72,82...|\n|(117,[1,3,72,82,8...|\n+--------------------+\nonly showing top 5 rows\n\n"
                }
            ], 
            "source": "train.select(\"features\").show(5)"
        }, 
        {
            "source": "**The input layer should have 117 nodes, the output layer should have 5 nodes (5 label categories). I add one hidden layer with 10 nodes to build the model**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 11, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "layers = Array(117, 10, 5)\nmlp = mlpc_7c6c1276365e\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 11, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "mlpc_7c6c1276365e"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "import org.apache.spark.ml.classification.MultilayerPerceptronClassifier\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\nval layers = Array[Int](117, 10, 5)\nval mlp = new MultilayerPerceptronClassifier()\n  .setLayers(layers)\n  .setBlockSize(128)\n  .setSeed(1234L)\n  .setMaxIter(100)"
        }, 
        {
            "source": "**Train the model (takes a bit more than 10 mins)**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 12, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Training process takes 753.023877001 secs\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "t = 1487954321358991\nmlp_model = mlpc_7c6c1276365e\nduration = 753.023877001\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 12, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "753.023877001"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "val t = System.nanoTime\nval mlp_model = mlp.fit(train)\nval duration = (System.nanoTime - t) / 1e9d\nprintln(s\"Training process takes $duration secs\")"
        }, 
        {
            "source": "**Evaluate the model, the result is pretty close to random forest**", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": 13, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [
                {
                    "output_type": "stream", 
                    "name": "stdout", 
                    "text": "Test Error of MLP = 0.008306689363822728\n"
                }, 
                {
                    "output_type": "display_data", 
                    "data": {
                        "text/plain": "mlp_predictions = [label: double, features: vector ... 1 more field]\naccuracy = 0.9916933106361773\n"
                    }, 
                    "metadata": {}
                }, 
                {
                    "execution_count": 13, 
                    "metadata": {}, 
                    "data": {
                        "text/plain": "0.9916933106361773"
                    }, 
                    "output_type": "execute_result"
                }
            ], 
            "source": "val mlp_predictions = mlp_model.transform(test)\nval accuracy = evaluator.evaluate(mlp_predictions)\nprintln(\"Test Error of MLP = \" + (1.0 - accuracy))"
        }, 
        {
            "source": "<a id=\"summary\"></a>\n## Summary and next steps     ", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "source": "**Two well-performing models are built in this notebook. It is very easy to build models using Spark API!**\n\n**There is no need to configure the Spark environment in Watson Studio. Just provision the Spark environment, create the notebook and you are ready to write your code!**\n\n**The speed of the Spark enviornment is good, especially when using Scala. I'll have another demo on using PySpark to build the exact same models with same environment.**\n\nNext steps:\n* Save/download the models to local\n* Save and deploy the models using Watson Machine Learning Service(WML)\n", 
            "cell_type": "markdown", 
            "metadata": {}
        }, 
        {
            "execution_count": null, 
            "cell_type": "code", 
            "metadata": {}, 
            "outputs": [], 
            "source": ""
        }
    ], 
    "metadata": {
        "kernelspec": {
            "display_name": "Scala 2.11 with Spark", 
            "name": "scala", 
            "language": "scala"
        }, 
        "language_info": {
            "mimetype": "text/x-scala", 
            "version": "2.11.8", 
            "name": "scala", 
            "pygments_lexer": "scala", 
            "file_extension": ".scala", 
            "codemirror_mode": "text/x-scala"
        }
    }, 
    "nbformat": 4
}